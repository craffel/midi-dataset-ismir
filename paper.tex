\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{framed}

\title{Large-Scale Content-Based Matching of MIDI and Audio Files}

%\twoauthors
%  {Colin Raffel} {LabROSA \\ Department of Electrical Engineering \\ Columbia University \\ New York, NY}
%  {Daniel P. W. Ellis} {LabROSA \\ Department of Electrical Engineering \\ Columbia University \\ New York, NY}

\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

\begin{document}

\maketitle

\begin{abstract}
  MIDI files, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks.
  We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content without using any metadata.
  The core of our approach is a neural network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space.
  Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings.
  We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset.
\end{abstract}

\section{Training Data for MIR}\label{sec:introduction}

Central to the task of content-based Music Information Retrieval is the curation of ground-truth data for tasks of interest (e.g. timestamped chord labels for automatic chord estimation, beat positions for beat tracking, prominent melody time series for melody extraction, etc.).
The quantity and quality of this ground-truth is often instrumental in the success of MIR systems which utilize it as training data.
Unfortunately, creating appropriate labels for a recording of a given song by hand typically requires person-hours on the order of the length of the song.
This often arguably makes the available training data a bottleneck to success for a given content-based MIR task.

It has previously been observed that MIDI files, when time-aligned to corresponding audio recordings, can be used to infer ground-truth information about a given song \cite{ewert2012towards, turetsky2003ground}.
This is due to the fact that a MIDI files can be viewed simplistically as a timed sequence of note annotations or a piano roll.
It is much more straightforward to estimate, e.g., beat locations, chord labels, and the predominant melody from these representations than one which was derived from an audio signal.
Unsurprisingly, a handful of tools have been developed for inferring this information from MIDI files \cite{eerola2004mir,mckay2006jsymbolic,cuthbert2010music21,raffel2014pretty_midi}.

In \cite{halevy2009unreasonable}, it is argued that some of the biggest successes in machine learning are thanks to the fact that ``...a large training set of the input-output behavior that we seek to automate is available to us in the wild.''
The main motivation behind this project is that this crucial availability of data holds true for MIDI files - through a large-scale web scrape, we obtained 140,910 unique MIDI files, which is orders of magnitude larger than the datasets typically used for MIR research.
We believe this proliferation of data is largely caused to two factors: First, that karaoke files are typically distributed as MIDI data and that karaoke is wildly popular, and second, that transcribing popular music as MIDI files is a common pastime for hobbyist musicians.

\subsection{Wrangling MIDI files}

The mere existence of a large collection of MIDI data is not enough, however.
As noted above, in order to use MIDI files as ground truth, they need to be both matched (paired with a corresponding audio recording of the same song) and aligned (adjusted so that the timing of the events transcribed in the file match the audio recording).
The latter problem has seen a great deal of research effort \cite{ewert2012towards, turetsky2003ground}, and will not be a main focus of this work.

Given large corpora of audio and MIDI files, the task of matching entries from each may seem to be a trivial problem involving fuzzy text matching of the files' metadata.
However, MIDI files have no formal mechanism for storing metadata (apart from text meta events, which are rarely used), and as a result the best-case scenario is that the artist and song title are included in the filename or subdirectory.
While we found some examples of this in our collection of scraped MIDI files, the vast majority of the files had effectively no metadata information.
Figure \ref{fig:midi-names} shows a random sampling of subdirectory and filenames from our collection.

\begin{figure}
  \begin{framed}
    \scriptsize
    \tt
    J/Jerseygi.mid

    V/VARIA18O.MID

    Carpenters/WeveOnly.mid

    2009 MIDI/handy\char`_man1-D105.mid

    G/Garotos Modernos - Bailanta De Fronteira.mid

    Various Artists/REWINDNAS.MID

    GoldenEarring/Twilight\char`_Zone.mid

    Sure.Polyphone.Midi/Poly 2268.mid

    d/danza3.mid

    100\%sure.polyphone.midi/Fresh.mid

    rogers\char`_kenny/medley.mid

    2009 MIDI/looking\char`_out\char`_my\char`_backdoor3-Bb192.mid
  \end{framed}

  \caption{Random sampling of 12 MIDI filenames and their parent directories from our corpus of 455,333 MIDI files scraped from the Internet.}
  \label{fig:midi-names}
\end{figure}

Fortunately, the goal of matching MIDI and audio files is to find pairs which have \textit{content} in common (i.e., the MIDI file is a transcription of the audio file), an information source which is available regardless of metadata quality.
However, comparing content has the potential to require much more computation than a fuzzy text comparison, and $NM$ comparisons must be made to match a MIDI dataset of size $N$ to an audio file dataset of size $M$.
Motivated by this, we propose a system which can \textit{efficiently} match MIDI files to audio based solely on their content.
Our system learns to hash MIDI and audio content to a common Hamming space where sequences of vectors can be compared efficiently using dynamic time warping (DTW).

The idea of using DTW distance to match MIDI files to audio recordings is not new.
For example, in \cite{hu2003polyphonic}, MIDI-audio matching is done by finding the minimal DTW distance between all pairs of chromagrams of (synthesized) MIDI and audio files.
Our approach differs in a few key ways: First, instead of using chromagrams (a hand-designed representation), we learn a common representation for MIDI and audio data.
Second, our datasets are many orders of magnitude larger (hundreds of thousands vs.\ hundreds of files), which necessitates a much more efficient approach.
Specifically, by mapping to a Hamming space we greatly speed up distance matrix calculation and we receive quadratic speed gains by implicitly downsampling the audio and MIDI feature sequences as part of our learned feature mapping.

In the following section, we detail the dataset of MIDI files we scraped from the Internet and describe how we prepared a subset for training our hasher.
Our cross-modality hashing model is then described in Section \ref{sec:hashing}.
In Section \ref{sec:dtw}, we cover our fast hash sequence DTW method and its accompanying pruning techniques.
Finally, we evaluate our system's performance on the task of matching files from our MIDI dataset to entries in the Million Song Dataset \cite{bertin2011million}.

\section{Preparing Data}
\label{sec:dataset}

Our project began with a large-scale scrape of MIDI files from the Internet.
We obtained 455,333 files, of which 140,910 were found to have unique MD5 checksums.
Most of these files were transcriptions of pieces of music of varying duration and quality.
As mentioned previously, the majority of these files had very little useful metadata information.
The goal of the present work is to develop an efficient way to match this corpus against the Million Song Dataset (MSD), or more specifically, to the short preview audio recordings provided by 7digital \cite{schindler2012facilitating}.

In order to evaluate our system, we need a collection of MIDI-audio pairs which we know are correctly matched.
The accuracy of our approach can then be judged based on how accurately it is able to recover these pairings using the content of the audio and MIDI files alone.
Fortunately, we identified a subset of files where the subdirectory of the file indicated the artist and the filename indicated the song title.
By cleaning up this subset and using the resulting metadata to match its entries against the Million Song Dataset, we can obtain the requisite ground-truth set of MIDI-audio pairings.
We will refer to this collection of files as the ``clean MIDI subset''.

A further data requirement for our system is a collection of feature vectors derived from audio and MIDI data which should be mapped to similar hashes.
This dataset will be used to train our model for hashing MIDI and audio features to a common Hamming space (described in Section \ref{sec:hashing}).
In our application, we will be matching sequences of hashes using dynamic time warping, so we need to obtain pairs of sequences of feature vectors where the $n$th vector in one sequence should be mapped to the $n$th vector in the other.
This necessitates a collection of MIDI files and audio recordings which we are confident are well-aligned in time.
From this collection, we can extract features for each modality and be sure that there is a direct correspondence between the resulting hash sequences and the original feature vector sequences.

\subsection{Metadata matching}

We first extracted the song title and artist from each entry in the clean MIDI subset based on each MIDI file's filename and subdirectory.
The resulting metadata was still somewhat messy; for example, ``The Beatles'' appeared as an artist along with ``Beatles, The'', ``Beatles'', and ``The Beatles John Paul Ringo George''.
To normalize these issues, we applied some manual text processing and resolved the artists and song titles against the Freebase \cite{bollacker2008freebase} and Echo Nest\footnote{\texttt{http://developer.echonest.com/docs/v4}} databases.
This resulted in 17,243 MIDI files for 10,060 unique songs.

As noted above, we will leverage the clean MIDI subset in two ways: First, to obtain ground-truth pairings of MSD/MIDI matches, and second, to create training data for our hashing scheme.
Note that for the latter purpose, we do not need to restrict ourselves to audio data from the MSD, and we can reasonably expect that by using more training data we may be able to learn a better representation.
As a result, we combined the MSD with three benchmark audio collections: CAL500 \cite{turnbull2007towards}, CAL10k \cite{tingle2010exploring}, and uspop2002 \cite{berenzweig2004large}.
To match these datasets to the clean MIDI subset, we used the Python search engine library Whoosh to perform a fuzzy matching of their metadata.
This resulted in 26,311 audio/MIDI file pairs corresponding to 5,243 unique songs.

\subsection{Synthesized MIDI-to-audio alignment}

Fuzzy metadata matching is not enough to ensure that we have correctly paired MIDI and audio files due to the following potential issues: The metadata could be incorrect, the fuzzy text match could have failed, the MIDI could be a very poor quality transcription (e.g. missing instruments or sections), and/or the MIDI and audio data could correspond to different versions of the same song.
These pitfalls necessitate a method to ensure the correctness of each MIDI/audio file pairing.
In additional, we need to align MIDI files to their corresponding audio recordings to use them as training data for our hashing model.
A common method for performing this kind of alignment is to compute the dynamic time warping (DTW) path between sequences of feature vectors \cite{turetsky2003ground, hu2003polyphonic,ewert2012towards}.
Conveniently, DTW reports a score which represents the quality of the alignment, and unsurprisingly this score tends to be very poor when non-matching sequences are attempted to be aligned.
An overview of DTW and its application to music can be found in \cite{muller2007information}.

For our purposes, the reliability of the DTW confidence score is crucial because we will use it to decide when an audio/MIDI file pairing is valid and when an alignment can be used as training data for our hashing model.
We experimented with a variety of approaches, and converged on the following system for aligning a single MIDI/audio file pair:
First, we synthesize the MIDI data using \texttt{fluidsynth}\footnote{\texttt{http://www.fluidsynth.org}}.
We then estimate the MIDI beat locations using the MIDI file's tempo change information and the method described in \cite{raffel2014pretty_midi}.
To circumvent the common issue where the beat is tracked one-half beat out of phase, we double the BPM until it was at least 240.
Using \texttt{librosa} \cite{mcfee2014librosa}, we compute beat locations for the audio signal with the constraint that the BPM should remain close to the global MIDI tempo. 
We then compute log-amplitude beat-synchronous constant-Q transforms (CQTs) of audio and synthesized MIDI data with semitone frequency spacing and a frequency range from C3 (65.4 Hz) to C7 (1046.5 Hz), also using \texttt{librosa}.
The resulting feature matrices are then of dimensionality $N \times D$ and $M \times D$ where $N$ and $M$ are the resulting number of beats in the MIDI and audio recordings respectively and $D$ is $48$ (the number of semitones between C3 and C7).

We then use dynamic time warping to find the lowest-cost path through a full pairwise cosine similarity matrix $S \in \mathbb{R}^{N \times M}$ of the MIDI and audio CQTs.
This path can be represented as two sequences $p, q \in \mathbb{R}^L$ of indices from each sequence such that $p[i] = n, q[i] = m$ implies that the $n$th MIDI beat should be aligned to the $m$th audio beat.
Traditional DTW requires that this path includes the start and end of each sequence, or equivalently that $p[1] = q[1] = 1; p[L] = N; q[L] = M$.
This approach is not valid when one sequence may be a subsequence of the other, which is often true in our problem setting due to the fact that, for example, the MSD 7digital audio recordings are cropped preview song clips.
We therefore modify this constraint so that either $gN \le p[L] \le N$ or $gM \le q[L] \le M$ where $g \approx 1$ is a parameter which provides a small amount of additional tolerance.
We also include the typical modification of applying an additive penalty for ``non-diagonal moves'', i.e. path entries where either $p[i] = p[i + 1]$ or $q[i] = q[i + 1]$.
For synthesized MIDI-to-audio alignment, we used $g = .95$ and set the non-diagonal additive penalty to the 90th percentile of $S$.

(Listening to alignments to find a threshold)

\subsection{Splitting the subset}

\section{Cross-Modality Hashing of MIDI and Audio Data}
\label{sec:hashing}

\section{Fast DTW Matching of Hash Sequences}
\label{sec:dtw}

\section{Matching MIDI Files to the MSD}
\label{sec:msd}

\bibliography{refs}

\end{document}
