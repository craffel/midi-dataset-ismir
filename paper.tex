\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}
\usepackage{framed}

\title{Large-Scale Content-Based Matching of MIDI and Audio Files}

%\twoauthors
%  {Colin Raffel} {LabROSA \\ Department of Electrical Engineering \\ Columbia University \\ New York, NY}
%  {Daniel P. W. Ellis} {LabROSA \\ Department of Electrical Engineering \\ Columbia University \\ New York, NY}

\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

\begin{document}

\maketitle

\begin{abstract}
  MIDI files, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks.
  We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content without using any metadata.
  The core of our approach is a neural network-based cross-modality hashing scheme which transforms acoustic feature matrices and MIDI piano rolls into sequences of vectors in a common Hamming space.
  Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings.
  We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset.
\end{abstract}

\section{Training Data for MIR}\label{sec:introduction}

Central to the task of content-based Music Information Retrieval is the curation of ground-truth data for tasks of interest (e.g. timestamped chord labels for automatic chord estimation, beat positions for beat tracking, prominent melody time series for melody extraction, etc.).
The quantity and quality of this ground-truth is often instrumental in the success of MIR systems which utilize it as training data.
Unfortunately, creating appropriate labels for a recording of a given song by hand often requires person-hours on the order of the length of the song.
This often arguably makes the available training data a bottleneck to success for a given content-based MIR task.

It has previously been observed that MIDI files, when time-aligned to corresponding audio recordings, can be used to infer ground-truth information about a given song \cite{ewert2012towards, turetsky2003ground}.
This is due to the fact that a MIDI files can be viewed simplistically as a timed sequence of note annotations or a piano roll.
It is much more straightforward to estimate, e.g., beat locations, chord labels, and the predominant melody from these representations than one which was derived from an audio signal.
Unsurprisingly, a handful of tools have been developed for inferring this information from MIDI files \cite{eerola2004mir,mckay2006jsymbolic,cuthbert2010music21,raffel2014pretty_midi}.

In \cite{halevy2009unreasonable}, it is argued that some of the biggest successes in machine learning are thanks to the fact that ``...a large training set of the input-output behavior that we seek to automate is available to us in the wild.''
The main motivation behind this project is that this crucial availability of data holds true for MIDI files - through a large-scale web scrape, we obtained 250,000 unique MIDI files, which is orders of magnitude larger than the datasets typically used for MIR research.
We believe this proliferation of data is largely caused to two factors: First, that karaoke files are typically distributed as MIDI data and that karaoke is wildly popular, and second, that transcribing popular music as MIDI files was a common pastime for hobbyist musicians in the nineties.

\subsection{Wrangling MIDI Files}

The mere existence of a large collection of MIDI data is not enough, however.
As noted above, in order to use MIDI files as ground truth, they need to be both matched (paired with a corresponding audio recording of the same song) and aligned (adjusted so that the timing of the events transcribed in the file match the audio recording).
The latter problem has seen a great deal of research effort \cite{ewert2012towards, turetsky2003ground}, and will not be a main focus of this work.

Given large corpora of audio and MIDI files, the task of matching entries from each may seem to be a trivial problem involving fuzzy text matching of the files' metadata.
However, MIDI files have no formal mechanism for storing metadata (apart from text meta events, which are rarely used), and as a result the best-case scenario is that the artist and song title are included in the filename or subdirectory.
While we found some examples of this in our collection of scraped MIDI files, the vast majority of the files had effectively no metadata information.
Figure \ref{fig:midi-names} shows a random sampling of subdirectory and filenames from our collection.

\begin{figure}
  \begin{framed}
    \scriptsize
    \tt
    J/Jerseygi.mid

    V/VARIA18O.MID

    Carpenters/WeveOnly.mid

    2009 MIDI/handy\char`_man1-D105.mid

    G/Garotos Modernos - Bailanta De Fronteira.mid

    Various Artists/REWINDNAS.MID

    GoldenEarring/Twilight\char`_Zone.mid

    Sure.Polyphone.Midi/Poly 2268.mid

    d/danza3.mid

    100\%sure.polyphone.midi/Fresh.mid

    rogers\char`_kenny/medley.mid

    2009 MIDI/looking\char`_out\char`_my\char`_backdoor3-Bb192.mid
  \end{framed}

  \caption{Random sampling of 12 MIDI filenames and their parent directories from our corpus of 250,000 MIDI files scraped from the Internet.}
  \label{fig:midi-names}
\end{figure}

Fortunately, the goal of matching MIDI and audio files is to find pairs which have \textit{content} in common (i.e., the MIDI file is a transcription of the audio file), an information source which is available regardless of metadata quality.
However, comparing content has the potential to require much more computation than a fuzzy text comparison, and $NM$ comparisons must be made to match a MIDI dataset of size $N$ to an audio file dataset of size $M$.
Motivated by this, we propose a system which can \textit{efficiently} match MIDI files to audio based solely on their content.
Our system learns to hash MIDI and audio content to a common Hamming space where sequences of vectors can be compared efficiently using dynamic time warping (DTW).

The idea of using DTW distance to match MIDI files to audio recordings is not new.
For example, in \cite{hu2003polyphonic}, MIDI-audio matching is done by finding the minimal DTW distance between all pairs of chromagrams of (synthesized) MIDI and audio files.
Our approach differs in a few key ways: First, instead of using chromagrams (a hand-designed representation), we optimize a common representation for MIDI and audio data.
This makes our system flexible with respect to the underlying feature representation used for each modality.
Second, our datasets are many orders of magnitude larger (hundreds of thousands vs.\ hundreds of files), which necessitates a much more efficient approach.
Specifically, by mapping to a Hamming space we greatly speed up distance matrix calculation and our proposed pruning techniques avoid a full DTW distance computation for the majority of file pairs.

In the following section, we detail the dataset of MIDI files we scraped from the Internet and describe how we prepared a subset for training our hasher.
Our cross-modality hashing model is then described in Section \ref{sec:hashing}.
In Section \ref{sec:dtw}, we cover our fast hash sequence DTW method and its accompanying pruning techniques.
Finally, we evaluate our system's performance on the task of matching files from our MIDI dataset to entries in the Million Song Dataset \cite{bertin2011million}.

\section{The MIDI Dataset}
\label{sec:dataset}

\section{Cross-Modality Hashing of MIDI and Audio Data}
\label{sec:hashing}

\section{Fast DTW Matching of Hash Sequences}
\label{sec:dtw}

\section{Matching MIDI Files to the MSD}
\label{sec:msd}

\bibliography{refs}

\end{document}
